{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing - part 2\n",
    "\n",
    "As stated in the 1st part, this notebook will deal with the definition and exemplification of A/B testing, crucial in day-to-day business operations, as well as Hypothesis Testing sensitivity. We will explore concepts such as Minimum Detectable Effect, CUPED, CUPAC and other metric analysis techninques useful in A/B testing and data-driven decision making.\n",
    "\n",
    "## Index:\n",
    "\n",
    "1. [A/B Testing](#1-ab-testing)\n",
    "2. [Minimum Detectable Effect](#2-minimum-detectable-effect-mde)\n",
    "3. [Improving Test sensibility](#3-improving-testing-sensitivity)\n",
    "   1. [CUPED & CUPAC](#31-cuped-controlled-experiment-using-pre-experiment-data-and-cupac-controlled-experiment-using-pre-assignment-covariates)\n",
    "   2. [Delta method and Ratio Metrics](#32-metric-analytics-delta-method-and-ration-metrics)\n",
    "4. [Extra Resources](#4-extra-resources)\n",
    "\n",
    "\n",
    "**Libraries used:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A/B Testing\n",
    "\n",
    "A/B testing is a form of two-sample (in this case) Z-Test, since it is normally used used to compare two versions of a product, website, ad, or feature to determine which one performs better (and therefore sample sizes are ussually large). It is widely used in marketing, UX design, and product optimization. \n",
    "\n",
    "We essentially compare Group A (control group), which is the baseline, and Group B, which is the modified version. Usually there is a division of users between the 2 groups randomly to avoid bias and then proceed with the collection of data and the conduction of this test. Metrics collected to use in this test include, for example:\n",
    "\n",
    "- Conversion Rate (CR): Percentage of users completing a desired action.\n",
    "- Click-Through Rate (CTR): Percentage of users who clicked on a link.\n",
    "- Bounce Rate: Percentage of users who leave without engaging.\n",
    "- Revenue per User: How much revenue is generated per visitor.\n",
    "\n",
    "The hypothesis are essentially the same as in every two-sample test: either there isn't or is a significant difference between the 2 versions. Since it is a Z-test (because of the sample size and we are testing for the proportions (nature of the metrics presented)), Central Limit Theorem is applicable to the score formula, which results in:\n",
    "\n",
    "$$\n",
    "Z = \\frac{p_B - p_A}{\\sqrt{SE_A^2 + SE_B^2}}\n",
    "$$\n",
    "\n",
    "where $p_A$ and $p_B$ are the observed metric, usually rates, since it is in percentage (**proportions**); $SE_A$ and $SE_B$ are are the observed conversion rates, and the denominator represents the standard error of the difference in proportions.\n",
    "\n",
    "Here is an example case:\n",
    "\n",
    "A company wants to test two call-to-action (CTA) buttons:\n",
    "\n",
    "- A (Control): \"Sign Up Now\"\n",
    "- B (Treatment): \"Get Started\"\n",
    "\n",
    "the test runs for a period of time, with the following results:\n",
    "\n",
    "| Group | Visitors | Conversions | Conversion Rate |\n",
    "|-------|----------|------------|----------------|\n",
    "| A     | 10,000  | 500        | 5%             |\n",
    "| B     | 10,000  | 600        | 6%             |\n",
    "\n",
    "This results in the following **Hypothesis Test**, with a significance level of 5%:\n",
    "\n",
    "- Null Hypothesis (H₀): No difference between A and B.\n",
    "- Alternative Hypothesis (H₁): A significant difference exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Rate A: 0.0500\n",
      "Conversion Rate B: 0.0600\n",
      "Z-score: 3.10\n",
      "p-value: 0.0019\n",
      "Conclusion: Reject H₀. There is a significant difference between the two variations.\n"
     ]
    }
   ],
   "source": [
    "# Data from the scenario\n",
    "n_A = 10000  # Number of visitors in group A\n",
    "n_B = 10000  # Number of visitors in group B\n",
    "conv_A = 500  # Conversions in group A\n",
    "conv_B = 600  # Conversions in group B\n",
    "\n",
    "# Compute conversion rates\n",
    "p_A = conv_A / n_A\n",
    "p_B = conv_B / n_B\n",
    "\n",
    "# Compute standard error for each group\n",
    "se_A = np.sqrt((p_A * (1 - p_A)) / n_A)\n",
    "se_B = np.sqrt((p_B * (1 - p_B)) / n_B)\n",
    "\n",
    "# Compute standard error of the difference\n",
    "se_diff = np.sqrt(se_A**2 + se_B**2)\n",
    "\n",
    "# Compute Z-score\n",
    "z_score = (p_B - p_A) / se_diff\n",
    "\n",
    "# Compute p-value (two-tailed test)\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "\n",
    "# Print results\n",
    "print(f\"Conversion Rate A: {p_A:.4f}\")\n",
    "print(f\"Conversion Rate B: {p_B:.4f}\")\n",
    "print(f\"Z-score: {z_score:.2f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Conclusion: Reject H₀. There is a significant difference between the two variations.\")\n",
    "else:\n",
    "    print(\"Conclusion: Fail to reject H₀. No significant difference between the variations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimum Detectable Effect (MDE)\n",
    "\n",
    "The Minimum Detectable Effect (MDE) is the smallest true effect size that a hypothesis test can detect with a given statistical power and significance level. Determining which MDE to use is important since it affects the needed sample size. A smaller MDE, meant for detecting small changes, needs a larger sample size, which results in higher traffic needs, increased time, and costs. On the other hand, a higher MDE has less sample size requirements but can potentially miss important smaller improvements. The general formula is the following:\n",
    "\n",
    "$$\n",
    "MDE = (Z_{1-\\alpha/2} + Z_{power}) \\times SE \\times \\sqrt{\\frac{2}{n}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Z_{1-\\alpha/2}$ is the critical value for the chosen confidence level.\n",
    "- $Z_{power}$ is is the z-score for the desired power.\n",
    "- $SE$ is the Standard Error\n",
    "- $n$ is the sample size per group\n",
    "\n",
    "Let's see a scenario where a company is running a test for a new landing page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Detectable Effect for the new landing page test: 0.0280\n"
     ]
    }
   ],
   "source": [
    "n = 5000  # Sample size per group\n",
    "alpha = 0.05  # Significance level\n",
    "power = 0.8  # Statistical power\n",
    "z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "z_power = stats.norm.ppf(power)\n",
    "se = np.sqrt((0.5 * (1 - 0.5)) / n)\n",
    "mde = (z_alpha + z_power) * se * np.sqrt(2)\n",
    "print(f\"Minimum Detectable Effect for the new landing page test: {mde:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On these conditions, the A/B testing can only flag significant changes if the new page results differs from the existing one at least 2.8%.\n",
    "\n",
    "## 3. Improving testing sensitivity.\n",
    "\n",
    "As we can see, the selection of a good MDE is dependant on human decision, and it can lead to possibly significant changes to not be detected, if they do not surpass the MDE threshold. To prevent this, we can improve the sensibility of the test, using tatics such as:\n",
    "\n",
    "- Increasing sample size.\n",
    "- Reducing measurement noise.\n",
    "- Using stratified sampling techniques.\n",
    "- Controlling for confounding variables (e.g., using CUPED).\n",
    "\n",
    "Here is a quick example, where we increase the sample size, based on the previous MDE calculation example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDE after doubling sample size: 0.0198. Now the threshold is smaller (1.9810%), and so the test will be more sensitive to flag significant changes.\n"
     ]
    }
   ],
   "source": [
    "original_sample_size = 5000\n",
    "new_sample_size = 10000  # Doubling the sample size\n",
    "new_mde = (z_alpha + z_power) * np.sqrt((0.5 * (1 - 0.5)) / new_sample_size) * np.sqrt(2)\n",
    "print(f\"MDE after doubling sample size: {new_mde:.4f}. Now the threshold is smaller ({new_mde*100:.4f}%), and so the test will be more sensitive to flag significant changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CUPED (Controlled-experiment Using Pre-Experiment Data) and CUPAC (Controlled-experiment Using Pre-Assignment Covariates)\n",
    "\n",
    "CUPED is a variance reduction technique that leverages pre-experiment data to improve test efficiency:\n",
    "\n",
    "$$\n",
    "Y_{adj} = Y - \\theta (X - \\bar{X})\n",
    "$$\n",
    "\n",
    "where $\\theta$ is calculated as the covariance between X and Y divided by the variance of X. Here is a quick example-application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before CUPED: 125.84\n",
      "Variance after CUPED: 24.64\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(100, 10, 5000)  # Pre-experiment data\n",
    "Y = X + np.random.normal(5, 5, 5000)  # Post-experiment metric\n",
    "\n",
    "theta = np.cov(X, Y)[0, 1] / np.var(X)\n",
    "Y_adj = Y - theta * (X - np.mean(X))\n",
    "\n",
    "print(f\"Variance before CUPED: {np.var(Y):.2f}\")\n",
    "print(f\"Variance after CUPED: {np.var(Y_adj):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUPAC extends CUPED by incorporating additional covariates that were known before assignment. These techninques help reduce variance further and increase test sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance after CUPAC: 118.21\n"
     ]
    }
   ],
   "source": [
    "covariate = np.random.normal(50, 10, 5000)\n",
    "Y_adj_cupac = Y_adj - theta * (covariate - np.mean(covariate))\n",
    "print(f\"Variance after CUPAC: {np.var(Y_adj_cupac):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Metric Analytics: Delta Method and Ration Metrics\n",
    "\n",
    "The Delta Method is used to approximate the variance of a function of a random variable, making it useful for constructing confidence intervals for non-linear metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance using Delta Method: 0.000009\n"
     ]
    }
   ],
   "source": [
    "original_metric = np.random.normal(1.5, 0.3, 5000)\n",
    "transformed_metric = np.log(original_metric)\n",
    "estimated_variance = np.var(transformed_metric) / len(original_metric)\n",
    "print(f\"Estimated variance using Delta Method: {estimated_variance:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio metrics (e.g., revenue per user) are commonly used in A/B testing. However, they require special handling to correctly interpret variance and statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test result for ratio metrics: T-statistic = -8.3164, p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hypothetical Scenario: Comparing revenue per user in an experiment\n",
    "group_A_revenue = np.random.normal(100, 20, 5000)\n",
    "group_B_revenue = np.random.normal(110, 20, 5000)\n",
    "ratio_A = group_A_revenue / np.random.randint(1, 5, 5000)\n",
    "ratio_B = group_B_revenue / np.random.randint(1, 5, 5000)\n",
    "\n",
    "# T-test for ratio metrics\n",
    "t_stat, p_value = stats.ttest_ind(ratio_A, ratio_B)\n",
    "print(f\"T-test result for ratio metrics: T-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extra Resources\n",
    "\n",
    "For further reading, to go more in depth about the topics shown:\n",
    "- [Minimum Detectable effect](https://splitmetrics.com/resources/minimum-detectable-effect-mde/)\n",
    "- [Improving Testing Sensitivity](https://kdd.org/kdd2016/papers/files/adp0945-xieA.pdf)\n",
    "- [CUPED](https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf)\n",
    "- [CUPAC](https://careersatdoordash.com/blog/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/)\n",
    "- [Delta Method](https://arxiv.org/pdf/1803.06336)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
